<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fundefined%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用word2vec训练词向量]]></title>
    <url>%2Fundefined%2Fnlp%2Fword2vec%2F</url>
    <content type="text"><![CDATA[将词转换为向量是目前自然语言处理的基本流程之一。词向量有好几种，本文主要讲述如何利用word2vec将中文词训练为分布式词向量。 语料准备本文使用维基百科文章作为训练语料。使用wget命令下载。 1wget https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2 下载完成后，应该得到一个名为 zhwiki-latest-pages-articles.xml.bz2 的文件。 这个文件是无法直接使用的，需要将其转换为文本，使用python中的gensim模块的WikiCorpus函数处理，完整代码如下： * wiki_to_txt.py 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import loggingimport sysreload(sys)sys.setdefaultencoding('utf-8')import iofrom gensim.corpora import WikiCorpusdef main(): if len(sys.argv) != 2: print("Usage: python3 " + sys.argv[0] + " wiki_data_path") exit() logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) wiki_corpus = WikiCorpus(sys.argv[1], dictionary=&#123;&#125;) texts_num = 0 with io.open("wiki_texts.txt", 'w', encoding='utf-8') as output: for text in wiki_corpus.get_texts(): output.write(b' '.join(text).decode('utf-8') + '\n') texts_num += 1 if texts_num % 10000 == 0: logging.info("已处理 %d 篇文章" % texts_num)if __name__ == "__main__": main() 执行：pyhton wiki_to_txt.py zhwiki-latest-pages-articles.xml.bz2进行处理。 处理结束后得到一个wiki_texts.txt文件。 查看一下这个文件： 发现文章是繁体字（应该是数据集下错了？），没关系，多加一步将繁体转换为中文： 首先，下载两个python文件：zh_wiki.py和langconv.py。 * t2s.py 123456789101112131415# -*- coding: utf-8 -*-from langconv import *def t2s(line): line = Converter('zh-hans').convert(line.decode('utf-8')) line = line.encode('utf-8') return lineif __name__ == "__main__": with open('wiki_texts.txt', 'r') as r: with open('wiki_zhs.txt', 'w') as w: lines = r.readlines() print 'start' for line in lines: w.write(t2s(line.rstrip() + '\n')) python t2s.py运行这个文件后得到简体中文的wiki数据集。处理结果存放在wiki_zhs.txt中。 分词和去除停止词使用 jieba 分词，在jieba_dict文件夹中的dict.txt.big文件可以自己定义一些想要分的词，在stopwords.txt中放入中文的停止词（可选）。完整代码如下： * segment.py 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import jiebaimport loggingimport ioimport sysreload(sys)sys.setdefaultencoding('utf-8')def main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # jieba custom setting. jieba.set_dictionary('jieba_dict/dict.txt.big') # load stopwords set stopword_set = set() with io.open('jieba_dict/stopwords.txt', 'r', encoding='utf-8') as stopwords: for stopword in stopwords: stopword_set.add(stopword.strip('\n')) output = io.open('wiki_seg.txt', 'w', encoding='utf-8') with io.open('wiki_zhs.txt', 'r', encoding='utf-8') as content: for texts_num, line in enumerate(content): line = line.strip('\n') words = jieba.cut(line, cut_all=False) for word in words: if word not in stopword_set: output.write(word + ' ') output.write(unicode('\n')) if (texts_num + 1) % 10000 == 0: logging.info("已完成前 %d 行的断词" % (texts_num + 1)) output.close()if __name__ == '__main__': main() 处理结果存放在wiki_seg.txt中，如下图： 训练词向量主要使用 gensim 中的 word2vec 训练词向量，完整代码如下： * train.py 123456789101112131415161718# -*- coding: utf-8 -*-import loggingfrom gensim.models import word2vecdef main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) sentences = word2vec.LineSentence("wiki_seg.txt") model = word2vec.Word2Vec(sentences, size=250) # 保存模型，供日后使用 model.save(u"word2vec.model") # 模型读取方式 # model = word2vec.Word2Vec.load("your_model_name")if __name__ == "__main__": main() 训练结果存储在word2vec.model中。 使用这里提供3种word2vec的测试场景：（1）一个词的相似词；（2）两个词的相似词；（3）类比推理。 完整代码如下： * demo.py 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding: utf-8 -*-from gensim.models import word2vecfrom gensim import modelsimport loggingdef main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) model = models.Word2Vec.load('word2vec.model') print("提供 3 种测试模式") print("输入一个词，则去寻找前一百个该词的相似詞") print("输入两个词，则去计算两个词的余弦相似度") print("输入三个词，进行类比推理") while True: query = raw_input("请输入: ") query = query.decode('utf-8') q_list = query.split() try: if len(q_list) == 1: print("相似词前 100 排序") res = model.most_similar(q_list[0], topn=100) for item in res: print(item[0] + "," + str(item[1])) elif len(q_list) == 2: print("计算 Cosine 相似度") res = model.similarity(q_list[0], q_list[1]) print(res) else: print("%s之于%s，如%s之于" % (q_list[0], q_list[2], q_list[1])) res = model.most_similar([q_list[0], q_list[1]], [q_list[2]], topn=100) for item in res: print(item[0] + "," + str(item[1])) print("----------------------------") except Exception as e: print(repr(e))if __name__ == "__main__": main()]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在mac上鼓捣终端]]></title>
    <url>%2Fundefined%2Fmac%2Fzsh%2F</url>
    <content type="text"><![CDATA[好看+好用 iTerm2mac必备，官网地址：http://iterm2.org/，或点击直接下载。 安装zsh一般mac会自带zsh，在命令行输入：zsh —version产看版本。 或者直接安装：brew install zsh（[Homebrew][https://brew.sh/]是mac的安装器）。 使zsh成为默认的shell：命令行输入chsh -s zsh，重启你的iTerm2。 安装Oh My Zsh通过wget安装，首先也要安装wget：brew install wget 在命令行输入： 1sh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" 主题oh my zsh自带各种主题，放在~/.oh-my-zsh/themes/文件夹下。 通过vi ~/.zshrc修改其中ZSH_THEME为你喜欢的主题（我比较喜欢默认主题）。 为你的vim安装Solarized或者Tomorrow主题： 1234567891011121314# git下载Solarized源码git clone git://github.com/altercation/solarized.git# 进入刚刚下载好的目录cd solarized/vim-colors-solarized/colors# 创建vim的配置文件夹sudo mkdir -p ~/.vim/colors# 把刚刚下载好的主题复制过去sudo cp solarized.vim ~/.vim/colors/# 创建.vimrc配置文件并修改sudo vim ~/.vimrc# 在.vimrc文件中加入以下几行syntax enableset background=darkcolorscheme solarized 保存.vimrc。 效果如下（不好意思，我用的是Tomorrow主题）： 什么！？怎么没有行号？怎么设置tab键的长短？怎么设置文件表头？ 好吧，给你完整的.vimrc（为vim添加行号、设置tab、设置新建*.sh和*.py文件时的表头）： 123456789101112131415161718192021222324252627282930313233syntax onset background=darkcolorscheme Tomorrow-Night-Eightiesset nu!set tabstop=4set ai!set showmatchset vb t_vb=set autoindentautocmd BufNewFile *.py,*.sh, exec ":call SetTitle()"let $author_name = "Your Name"let $author_email = "Your E-mail"func SetTitle() if &amp;filetype == 'sh' call setline(1,"\#======================================================= ================") call append(line("."), "\# File Name: ".expand("%")) call append(line(".")+1, "\# Author: ".$author_name) call append(line(".")+2, "\# mail: ".$author_email) call append(line(".")+3, "\# Created Time: ".strftime("%c")) call append(line(".")+4, "\#============================================= ================") call append(line(".")+5, "\#!/bin/zsh") call append(line(".")+6, "") else call setline(1, "\# -*- coding: utf-8 -*-") call append(line("."),"\"\"\"") call append(line(".")+1, "\Created Time: ".strftime("%T %x")) call append(line(".")+2, "") call append(line(".")+3, "\Author: ".$author_name) call append(line(".")+4, "\"\"\"") endifendfunc 插件Oh My Zsh支持很多强大的插件，例如git、语法高亮、命令补全等（不过不要加太多的插件，否则会拖慢shell的启动）。 1234567# 下载zsh-syntax-highlighting插件cd ~/.oh-my-zsh/custom/pluginsgit clone git://github.com/zsh-users/zsh-syntax-highlighting.git# 打开.zshrcvi ~/.zshrc# 找到plugins，将你想要的插件添加进去，插件之间以空格隔开plugins=(zsh-syntax-highlighting zsh-autosuggestions git pip python) 保存.zshrc并退出，重启终端或者输入source ~/.zshrc或zsh使配置文件生效。]]></content>
      <categories>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>terminal</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
</search>
